{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf4a38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: {0}\n",
      "Assistant: <think>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "train: datasets.Dataset = ds[\"train\"]\n",
    "prompt_templ = \"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "User: {0}\n",
    "Assistant: <think>\"\"\"\n",
    "print(prompt_templ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c199f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_alignment.math_baseline import extract_answer\n",
    "prompts = []\n",
    "ground_truths = []\n",
    "for t, data in enumerate(train):\n",
    "    question = data[\"question\"]\n",
    "    answer_text = data[\"answer\"]\n",
    "    answer = extract_answer(answer_text)\n",
    "    assert answer is not None, f\"Could not extract answer from: {answer_text}\"\n",
    "    full_prompt = prompt_templ.format(question)\n",
    "    prompts.append(full_prompt)\n",
    "    ground_truths.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aef9369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Rica's group won in a dance competition. She got 3/8 of the prize money. From Rica's prize money, she spent 1/5 of it and is now left with $300. How much was the prize money that her group won?\n",
      "--------------------\n",
      "Ground truth answer:\n",
      "1000\n",
      "--------------------\n",
      "Answer Text:\n",
      "Rica is left with 1 - 1/5 = 4/5 of her prize money which is equal to $300.\n",
      "Since 4/5 is worth $300, then 1/5 is worth $300/4 = $75.\n",
      "So, Rica got $75 x 5 = $375 from their prize money which is 3/8 of the total prize.\n",
      "Since 3/8 is equal to $375, then 1/8 is worth $375/3 = $125.\n",
      "So, the total prize money is $125 x 8 = $<<125*8=1000>>1000.\n",
      "#### 1000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0, len(prompts) - 1)\n",
    "print(\"Question:\")\n",
    "print(train[index][\"question\"])\n",
    "print(\"-\" * 20)\n",
    "print(\"Ground truth answer:\")\n",
    "print(ground_truths[index])\n",
    "print(\"-\" * 20)\n",
    "print(\"Answer Text:\")\n",
    "print(train[index][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d943bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/math_baseline_eval_results.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0153712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: A bulk warehouse is offering 48 cans of sparkling water for $12.00 a case.  The local grocery store is offering the same sparkling water for $6.00 and it only has 12 cans.  How much more expensive, per can, in cents, is this deal at the grocery store?\n",
      "Assistant: <think>\n",
      "--------------------\n",
      "Completion:\n",
      " This question can be approached by calculating the price per can at each store and comparing\n",
      "--------------------\n",
      "Ground Truth:\n",
      "25\n",
      "--------------------\n",
      "Reward:\n",
      "{'format_reward': 0.0, 'answer_reward': 0.0, 'reward': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "prompts = results[\"prompts\"]\n",
    "completions = results[\"completions\"]\n",
    "rewards = results[\"rewards\"]\n",
    "ground_truths = results[\"ground_truths\"]\n",
    "\n",
    "index = random.randint(0, len(prompts) - 1)\n",
    "print(\"Prompt:\")\n",
    "print(prompts[index])\n",
    "print(\"-\" * 20)\n",
    "print(\"Completion:\")\n",
    "print(completions[index])\n",
    "print(\"-\" * 20)\n",
    "print(\"Ground Truth:\")\n",
    "print(ground_truths[index])\n",
    "print(\"-\" * 20)\n",
    "print(\"Reward:\")\n",
    "print(rewards[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
