{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "train: datasets.Dataset = ds[\"train\"]\n",
    "prompt_templ = \"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "User: {0}\n",
    "Assistant: <think>\"\"\"\n",
    "print(prompt_templ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_alignment.math_baseline import extract_answer\n",
    "prompts = []\n",
    "ground_truths = []\n",
    "for t, data in enumerate(train):\n",
    "    question = data[\"question\"]\n",
    "    answer_text = data[\"answer\"]\n",
    "    answer = extract_answer(answer_text)\n",
    "    assert answer is not None, f\"Could not extract answer from: {answer_text}\"\n",
    "    full_prompt = prompt_templ.format(question)\n",
    "    prompts.append(full_prompt)\n",
    "    ground_truths.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index = random.randint(0, len(prompts) - 1)\n",
    "print(\"Question:\")\n",
    "print(train[index][\"question\"])\n",
    "print(\"-\" * 20)\n",
    "print(\"Ground truth answer:\")\n",
    "print(ground_truths[index])\n",
    "print(\"-\" * 20)\n",
    "print(\"Answer Text:\")\n",
    "print(train[index][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d943bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from cs336_alignment.math_baseline import EvalEntry\n",
    "with open(\"../data/math_baseline_eval_results.pkl\", \"rb\") as f:\n",
    "    results: list[EvalEntry] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f0153712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Format Reward looks correct, only one issue is that therre are some cases \\nwhere <think/> something <answer> answer </answer> is not satisfied. \\nWe can improve the format reward by checking for this condition.'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from cs336_alignment.math_baseline import EvalEntry\n",
    "total_samples = len(results)\n",
    "index = random.randint(0, total_samples - 1)\n",
    "eval_entry: EvalEntry = results[index]\n",
    "if eval_entry.reward <= 0 and eval_entry.format_reward <= 0:\n",
    "    print(\"Prompt:\")\n",
    "    print(eval_entry.prompt)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Response:\")\n",
    "    print(eval_entry.response)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Ground Truth:\")\n",
    "    print(eval_entry.ground_truth)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Reward:\")\n",
    "    print(eval_entry.reward)\n",
    "\n",
    "\"\"\"Format Reward looks correct, only one issue is that therre are some cases \n",
    "where <think/> something <answer> answer </answer> is not satisfied. \n",
    "We can improve the format reward by checking for this condition.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "567f85b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 32\n",
      "Correct samples: 7, percent: 0.2188\n",
      "Format correct samples: 0, percent: 0.0000\n",
      "Wrong samples: 25, percent: 0.7812\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "format_correct_count = 0\n",
    "wrong_count = 0\n",
    "total_count = len(results)\n",
    "for t, entry in enumerate(results):\n",
    "    if entry.reward > 0:\n",
    "        correct_count += 1\n",
    "    elif entry.format_reward > 0:\n",
    "        format_correct_count += 1\n",
    "    else:\n",
    "        wrong_count += 1\n",
    "print(f\"Total samples: {len(results)}\")\n",
    "print(f\"Correct samples: {correct_count}, percent: {correct_count / total_count:.4f}\")\n",
    "print(f\"Format correct samples: {format_correct_count}, percent: {format_correct_count / total_count:.4f}\")\n",
    "print(f\"Wrong samples: {wrong_count}, percent: {wrong_count / total_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c396be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total such samples: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Cannot choose from an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m         all_indexes.append(t)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal such samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_indexes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m index = \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_indexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m entry = results[index]\n\u001b[32m     11\u001b[39m format_rewards = entry.format_reward\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/random.py:347\u001b[39m, in \u001b[36mRandom.choice\u001b[39m\u001b[34m(self, seq)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# As an accommodation for NumPy, we don't use \"if not seq\"\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# because bool(numpy.array()) raises a ValueError.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seq):\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mCannot choose from an empty sequence\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m seq[\u001b[38;5;28mself\u001b[39m._randbelow(\u001b[38;5;28mlen\u001b[39m(seq))]\n",
      "\u001b[31mIndexError\u001b[39m: Cannot choose from an empty sequence"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from math_verify import parse\n",
    "all_indexes = []\n",
    "for t, entry in enumerate(results):\n",
    "    if entry.answer_reward_v2 > 0 and entry.format_reward > 0 and entry.reward <= 0:\n",
    "        all_indexes.append(t)\n",
    "\n",
    "print(f\"Total such samples: {len(all_indexes)}\")\n",
    "index = random.choice(all_indexes)\n",
    "entry = results[index]\n",
    "format_rewards = entry.format_reward\n",
    "print(\"Prompt:\")\n",
    "print(entry.prompt)\n",
    "print(\"-\" * 20)\n",
    "print(\"Response:\")\n",
    "print(parse(entry.response))\n",
    "print(\"-\" * 20)\n",
    "print(\"Ground Truth:\")\n",
    "print(parse(entry.ground_truth))\n",
    "print(\"-\" * 20)\n",
    "print(\"Reward:\")\n",
    "print(f\"reward: {entry.reward}, format_reward: {entry.format_reward}, answer_reward_v2: {entry.answer_reward_v2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3066de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp_answer: [0, '0'], ground truth: [Eq(-6, 0) & Eq(2*a, -6), '2a = -6 = 0'], is_correct: 1.0\n",
      "Reward: {'format_reward': 1.0, 'answer_reward': 0.0, 'reward': 0.0}\n",
      "is_latex_equal: True\n"
     ]
    }
   ],
   "source": [
    "from cs336_alignment.drgrpo_grader import r1_zero_reward_fn, _normalize\n",
    "from math_verify import parse, verify, LatexExtractionConfig, ExprExtractionConfig\n",
    "\n",
    "resp_answer = parse(entry.response)\n",
    "gt_answer = parse(entry.ground_truth)\n",
    "is_correct = verify(resp_answer, gt_answer)\n",
    "print(f\"resp_answer: {resp_answer}, ground truth: {gt_answer}, is_correct: {1.0 if is_correct else 0.0}\")\n",
    "\n",
    "reward = r1_zero_reward_fn(entry.response, entry.ground_truth, False)\n",
    "print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9c132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "ds = load_dataset(\"hkust-nlp/dart-math-uniform\")\n",
    "train: datasets.Dataset = ds[\"train\"]\n",
    "prompt_templ = \"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "User: {0}\n",
    "Assistant: <think>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from cs336_alignment.extract import extract_ans\n",
    "prompts: List[str] = []\n",
    "responses: List[str] = []\n",
    "for t, data in enumerate(train):\n",
    "    question: str = data[\"query\"] # type: ignore\n",
    "    answer_text: str = data[\"response\"] # type: ignore\n",
    "    answer = extract_ans(answer_text, True)\n",
    "    if answer is None:\n",
    "        print(f\"Skipping sample {t} due to no extractable answer.\")\n",
    "        break\n",
    "    full_prompt = prompt_templ.format(question)\n",
    "    prompts.append(full_prompt)\n",
    "    responses.append(f\"{answer_text} </think> <answer> {answer} </answer>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb29a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp_answer: [11/850, '\\\\frac{11}{850}']\n"
     ]
    }
   ],
   "source": [
    "from math_verify import parse, verify\n",
    "import random\n",
    "index = random.randint(0, len(train) - 1)\n",
    "data = train[index]\n",
    "\n",
    "response = data[\"response\"]\n",
    "\n",
    "resp_answer = parse(response)\n",
    "print(f\"resp_answer: {resp_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18c9ea89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11/850'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(resp_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bdb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
