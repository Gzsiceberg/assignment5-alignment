[project]
name = "alignment"
version = "1.0.0"
description = "CS 336 Spring 2025 Assignment 5: Alignment"
readme = "README.md"
requires-python = ">=3.11,<3.13"  # Python 3.13 not yet supported for some deps
dependencies = [
    "accelerate>=1.5.2",
    "alpaca-eval",
    "flash-attn==2.7.4.post1",
    "jupyter>=1.1.1",
    "math-verify[antlr4-13-2]>=0.7.0",
    "pylatexenc==2.10",
    "notebook>=7.4.2",
    "pytest>=8.3.5",
    "torch",
    "tqdm>=4.67.1",
    "transformers>=4.50.0",
    "typer>=0.15.4",
    "vllm==0.7.2",
    "wandb>=0.19.8",
    "xopen>=2.0.2",
    "black>=25.9.0",
    "datasets>=2.19",
    "fsspec>=2025.3.2",
    "huggingface-hub>=0.31.1",
    "jaxtyping>=0.3.3",
]

[tool.setuptools.packages.find]
include = ["cs336_alignment"]

[tool.uv]
package = true
no-build-isolation-package = ["flash-attn"]


[tool.uv.sources]
alpaca-eval = { git = "https://github.com/nelson-liu/alpaca_eval.git", rev = "forward_kwargs_to_vllm" }
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl" }

[tool.pytest.ini_options]
log_cli = true
log_cli_level = "WARNING"

[[tool.uv.dependency-metadata]]
name = "flash-attn"
version = "2.7.4.post1"
requires-dist = ["torch", "einops", "setuptools"]
